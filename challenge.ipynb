{"cells":[{"cell_type":"code","source":["%sql\nCREATE DATABASE IF NOT EXISTS db_bronze;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"8a0d9f4c-d4e1-451b-ba6b-ac0a7899d881","inputWidgets":{},"title":"CREAR BASE DE DATOS"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDROP TABLE IF EXISTS db_bronze.hired_employees;\nCREATE TABLE db_bronze.hired_employees(\n  id bigint,\n  name string,\n  datetime string,\n  department_id bigint,\n  job_id bigint\n);\n\nDROP TABLE IF EXISTS db_bronze.departments;\nCREATE TABLE db_bronze.departments(\n  id bigint,\n  department string\n);\n\nDROP TABLE IF EXISTS db_bronze.jobs;\nCREATE TABLE db_bronze.jobs(\n  id bigint,\n  job string\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"9eddcb7e-ab05-46bc-8a58-2000c14738a1","inputWidgets":{},"title":"CREACION DE TABLAS"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%python\n# coding: utf-8\n\n# ||********************************************************************************************************\n# || PROYECTO   \t\t: POC -CHALLENGE GLOBLANT \n# || NOMBRE     \t\t: challenge.py\n# || TABLA DESTINO\t: db_bronze.hired_employees\n# ||                  db_bronze.departments \n# ||                  db_bronze.jobs\n# || TABLAS FUENTES\t: departments.csv\n# ||                  hired_employees.csv\n# ||                  jobs.csv\n# || OBJETIVO   \t\t: ETL - big data migrati\n# || TIPO       \t\t: pyspark\n# || REPROCESABLE\t  : NA\n# || SCHEDULER\t\t  : NA\n# || JOB  \t\t      : NA\n# || VERSION   DESARROLLADOR           FECHA        DESCRIPCION\n# || 1.1       ALEXIS DAVILA        21/03/23     Creacion del proceso\n# *************************************************************************************************************\n\n###\n # @section Import\n ##\n\nimport findspark\nfindspark.init()\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\n###\n # @section configuracion de recursos\n ##\nspark = SparkSession.builder.\\\nappName(\"CSV to Database\").\\\nconfig(\"spark.driver.cores\",\"8\").\\\nconfig(\"spark.driver.memoryOverhead\",\"20g\").\\\nconfig(\"spark.driver.memory\", \"16g\").\\\nconfig(\"spark.executor.cores\",\"15\").\\\nconfig(\"spark.executor.memory\",\"20g\").\\\nconfig(\"spark.executor.memoryOverhead\",\"20g\").\\\nconfig(\"spark.dynamicAllocation.enabled\",\"true\").\\\nconfig(\"spark.dynamicAllocation.maxExecutors\",\"20\").\\\nconfig(\"spark.debug.maxToStringFields\", \"100\").\\\nconfig(\"spark.shuffle.service.enabled\",\"true\").\\\nconfig(\"spark.sql.shuffle.partitions\", \"500\").\\\nconfig(\"spark.default.parallelism\", \"500\").\\\nconfig(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\").\\\nconfig(\"spark.sql.join.preferSortMergeJoin\",\"false\").\\\nconfig(\"spark.maxRemoteBlockSizeFetchToMem\",\"2147483135\").\\\nconfig(\"spark.shuffle.spill.compress\",\"true\").\\\nconfig(\"spark.shuffle.compress\",\"true\").\\\nconfig(\"spark.ui.enabled\",\"true\").\\\nenableHiveSupport().\\\ngetOrCreate()\n\n###\n # @section funciones\n ##\n\n\ndef insert_csv_to_db(csv_file,table_name,nombre_columnas):\n    df = spark.read.format(\"csv\").option(\"header\", \"false\").option(\"inferSchema\",\"True\").load(csv_file)\n    nombres_columnas_final = nombre_columnas\n    df = df.toDF(*nombres_columnas_final)\n    df.write.format(\"avro\").mode(\"append\").option(\"batchsize\", \"1000\").option(\"isolationLevel\", \"NONE\").option(\"numPartitions\", \"10\").option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n\ndef main():\n    \n    csv_file_1 = \"https://raw.githubusercontent.com/alexis18daes/databricks_challenge_repo/dev/hired_employees.csv\" \n    csv_file_2 = \"https://raw.githubusercontent.com/alexis18daes/databricks_challenge_repo/dev/departments.csv\"\n    csv_file_3 = \"https://raw.githubusercontent.com/alexis18daes/databricks_challenge_repo/dev/jobs.csv\"\n    \n    table_name_1 = \"db_bronze.hired_employees\"\n    table_name_2 = \"db_bronze.departments\"\n    table_name_3 = \"db_bronze.jobs\"\n    \n    nombre_columnas_1 = [\"id\",\"name\",\"datetime\",\"department_id\",\"job_id\"]\n    nombre_columnas_2 = [\"id\",\"name\",\"department\"]\n    nombre_columnas_3 = [\"id\",\"job\"]\n    \n    insert_csv_to_db(csv_file_1,table_name_1,nombre_columnas_1)\n    insert_csv_to_db(csv_file_2,table_name_2,nombre_columnas_2)\n    insert_csv_to_db(csv_file_3,table_name_3,nombre_columnas_3)\n    \n#Ejecucion\nmain()\n\nspark.stop()\n\n#Salida\nexit()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"c1b54a92-42e6-4612-84d2-982adf2f02eb","inputWidgets":{},"title":"PROCESO PRINCIPAL"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"challenge","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":882737275823809}},"nbformat":4,"nbformat_minor":0}
