{"cells":[{"cell_type":"code","source":["%sql\nCREATE DATABASE IF NOT EXISTS db_bronze;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"implicitDf":true},"nuid":"8a0d9f4c-d4e1-451b-ba6b-ac0a7899d881","inputWidgets":{},"title":"CREAR BASE DE DATOS"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nDROP TABLE IF EXISTS db_bronze.table_hired_employees;\nCREATE TABLE db_bronze.table_hired_employees(\n  id bigint,\n  name string,\n  datetime string,\n  department_id bigint,\n  job_id bigint\n);\n\nDROP TABLE IF EXISTS db_bronze.table_departments;\nCREATE TABLE db_bronze.table_departments(\n  id bigint,\n  department string\n);\n\nDROP TABLE IF EXISTS db_bronze.table_jobs;\nCREATE TABLE db_bronze.table_jobs(\n  id bigint,\n  job string\n)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"implicitDf":true},"nuid":"9eddcb7e-ab05-46bc-8a58-2000c14738a1","inputWidgets":{},"title":"CREACION DE TABLAS"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%python\n# coding: utf-8\n\n# ||********************************************************************************************************\n# || PROYECTO   \t\t: POC -CHALLENGE GLOBLANT \n# || NOMBRE     \t\t: challenge.py\n# || TABLA DESTINO\t: db_bronze.hired_employees\n# ||                  db_bronze.departments \n# ||                  db_bronze.jobs\n# || TABLAS FUENTES\t: departments.csv\n# ||                  hired_employees.csv\n# ||                  jobs.csv\n# || OBJETIVO   \t\t: ETL - big data migrati\n# || TIPO       \t\t: pyspark\n# || REPROCESABLE\t  : NA\n# || SCHEDULER\t\t  : NA\n# || JOB  \t\t      : NA\n# || VERSION   DESARROLLADOR           FECHA        DESCRIPCION\n# || 1.1       ALEXIS DAVILA        21/03/23     Creacion del proceso\n# *************************************************************************************************************\n\n###\n # @section Import\n ##\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType,DoubleType,FloatType,LongType\n\n###\n # @section configuracion de recursos\n ##\nspark = SparkSession.builder.appName(\"CSV to Database\").getOrCreate()\n\n###\n # @section funciones\n ##\n\n\ndef insert_csv_to_db(csv_file,table_name,nombre_columnas):\n    \"\"\"\"\n    Esta función inserta data de archivos csv a una base de datos especificando las columnas.\n    :param csv_file: archivos csv\n    :param table_name: nombre y esquema de la tabla en donde se inserta la data\n    :param nombre_columnas: la lista de columnas de la tabla\n    \"\"\"\n    # Lee la tabla y guarda su contenido en un DataFrame\n    data_test = pd.read_csv(csv_file,sep=',', header='infer')\n    df=spark.createDataFrame(data_test)\n    nombres_columnas_final = nombre_columnas\n    #Asigna las columnas en el dataframe\n    df = df.toDF(*nombres_columnas_final)\n    department_id='department_id'\n    if department_id in nombres_columnas_final:\n        df = df.withColumn(\"department_id\",col(\"department_id\").cast(LongType()))\n        df = df.withColumn(\"job_id\",col(\"job_id\").cast(LongType()))\n    else:\n        print('Argumento \"valida id\" no válido')\n    # Guarda el DataFrame en formato delta en la la tabla especificada\n    df.show()\n    df.write.format(\"delta\").mode(\"overwrite\").option(\"batchsize\", \"1000\").option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n\ndef backup_table(table_name, backup_path, backup_format=\"avro\"):\n    \"\"\"\n    Esta función crea una copia de seguridad de una tabla en formato AVRO y la guarda en el sistema de archivos.\n    :param table_name: nombre de la tabla a respaldar\n    :param backup_path: ruta donde se guardará el archivo de respaldo\n    :param backup_format: formato en que se guardará el archivo de respaldo. Por defecto es \"avro\".\n    \"\"\"\n    # Lee la tabla y guarda su contenido en un DataFrame\n    df = spark.table(table_name)\n    # Guarda el DataFrame en formato AVRO en la ruta especificada\n    df.write.format(backup_format).save(backup_path)\n    \ndef restore_table(table_name, backup_path, backup_format=\"avro\"):\n    \"\"\"\n    Esta función restaura una tabla a partir de su copia de seguridad en formato AVRO.\n    :param table_name: nombre de la tabla a restaurar\n    :param backup_path: ruta donde se encuentra el archivo de copia de seguridad\n    :param backup_format: formato del archivo de copia de seguridad. Por defecto es \"avro\".\n    \"\"\"\n    # Lee el archivo de copia de seguridad en formato AVRO y carga su contenido en un DataFrame\n    df = spark.read.format(backup_format).load(backup_path)\n    # Escribe el contenido del DataFrame en la tabla especificada\n    df.write.mode(\"overwrite\").saveAsTable(table_name)\n\ndef main():\n    \n    csv_file_1 = \"https://raw.githubusercontent.com/alexis18daes/databricks_challenge_repo/dev/hired_employees.csv\" \n    csv_file_2 = \"https://raw.githubusercontent.com/alexis18daes/databricks_challenge_repo/dev/departments.csv\"\n    csv_file_3 = \"https://raw.githubusercontent.com/alexis18daes/databricks_challenge_repo/dev/jobs.csv\"\n    \n    table_name_1 = \"db_bronze.table_hired_employees\"\n    #table_name_2 = \"db_bronze.table_departments\"\n    #table_name_3 = \"db_bronze.table_jobs\"\n    \n    nombre_columnas_1 = [\"id\",\"name\",\"datetime\",\"department_id\",\"job_id\"]\n    #nombre_columnas_2 = [\"id\",\"department\"]\n    #nombre_columnas_3 = [\"id\",\"job\"]\n    \n    insert_csv_to_db(csv_file_1,table_name_1,nombre_columnas_1)\n    #insert_csv_to_db(csv_file_2,table_name_2,nombre_columnas_2)\n    #insert_csv_to_db(csv_file_3,table_name_3,nombre_columnas_3)\n    \n#Ejecucion\nmain()\n\n#spark.stop()\n\n#Salida\n#exit()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"c1b54a92-42e6-4612-84d2-982adf2f02eb","inputWidgets":{},"title":"CHALLENGE #1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"java.lang.RuntimeException: abort: DriverClient destroyed\n\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$3(DriverClient.scala:602)\n\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat com.databricks.threading.NamedExecutor$$anon$2.$anonfun$run$1(NamedExecutor.scala:362)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:415)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n\tat com.databricks.threading.NamedExecutor.withAttributionContext(NamedExecutor.scala:289)\n\tat com.databricks.threading.NamedExecutor$$anon$2.run(NamedExecutor.scala:360)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n","errorSummary":"Internal error. Attach your notebook to a different cluster or restart the current cluster.","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.lang.RuntimeException: abort: DriverClient destroyed\n\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$3(DriverClient.scala:602)\n\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat com.databricks.threading.NamedExecutor$$anon$2.$anonfun$run$1(NamedExecutor.scala:362)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:415)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n\tat com.databricks.threading.NamedExecutor.withAttributionContext(NamedExecutor.scala:289)\n\tat com.databricks.threading.NamedExecutor$$anon$2.run(NamedExecutor.scala:360)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n--Number of employees hired for each job and department in 2021 divided by quarter. The table must be ordered alphabetically by department and job.\n\n\nSELECT x.department, x.job,\nCOUNT(CASE WHEN DATEPART(QUARTER, x.final_date) = 1 THEN id END) AS Q1,\nCOUNT(CASE WHEN DATEPART(QUARTER, x.final_date) = 2 THEN id END) AS Q2,\nCOUNT(CASE WHEN DATEPART(QUARTER, x.final_date) = 3 THEN id END) AS Q3,\nCOUNT(CASE WHEN DATEPART(QUARTER, x.final_date) = 4 THEN id END) AS Q4\nFROM (\nSELECT a.id,a.name, cast(to_timestamp('second',a.datetime::timestamp) as date) as final_date, b.department,c.job\nfrom db_bronze.table_hired_employees a\nleft join db_bronze.table_departments b\non a.department_id=b.id\nleft join db_bronze.table_jobs c\non a.job_id=c.id) x\nWHERE YEAR(x.final_date) = 2021\nGROUP BY x.department, x.job\nORDER BY x.department, x.job;\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"implicitDf":true},"nuid":"4ffa119f-2364-4b6b-9022-69d4087310b5","inputWidgets":{},"title":"CHALLENGE #2.1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"java.lang.RuntimeException: abort: DriverClient destroyed\n\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$3(DriverClient.scala:602)\n\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat com.databricks.threading.NamedExecutor$$anon$2.$anonfun$run$1(NamedExecutor.scala:362)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:415)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n\tat com.databricks.threading.NamedExecutor.withAttributionContext(NamedExecutor.scala:289)\n\tat com.databricks.threading.NamedExecutor$$anon$2.run(NamedExecutor.scala:360)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n","errorSummary":"Internal error. Attach your notebook to a different cluster or restart the current cluster.","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.lang.RuntimeException: abort: DriverClient destroyed\n\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$3(DriverClient.scala:602)\n\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat com.databricks.threading.NamedExecutor$$anon$2.$anonfun$run$1(NamedExecutor.scala:362)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:415)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n\tat com.databricks.threading.NamedExecutor.withAttributionContext(NamedExecutor.scala:289)\n\tat com.databricks.threading.NamedExecutor$$anon$2.run(NamedExecutor.scala:360)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n--List of ids, name and number of employees hired of each department that hired more\n--employees than the mean of employees hired in 2021 for all the departments, ordered\n--by the number of employees hired (descending).\n\nSELECT x.department, COUNT(x.id) as hired\nFROM (\nSELECT a.id,a.name, cast(to_timestamp('second',a.datetime::timestamp) as date) as final_date, b.department,c.job\nfrom db_bronze.table_hired_employees a\nleft join db_bronze.table_departments b\non a.department_id=b.id\nleft join db_bronze.table_jobs c\non a.job_id=c.id\n) x\nWHERE YEAR(x.final_date) = 2021\nGROUP BY x.department\nHAVING COUNT(x.id) > (SELECT AVG(d.hired) FROM \n(SELECT z.department, COUNT(z.id) as hired\nFROM(\nSELECT a.id,a.name, cast(to_timestamp('second',a.datetime::timestamp) as date) as final_date, b.department,c.job\nfrom db_bronze.table_hired_employees a\nleft join db_bronze.table_departments b\non a.department_id=b.id\nleft join db_bronze.table_jobs c\non a.job_id=c.id\n) z\nWHERE YEAR(z.final_date) = 2021\nGROUP BY z.department) d)\nORDER BY hired DESC;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"f3be2ca2-1005-4c75-83db-71a8f296244c","inputWidgets":{},"title":"CHALLENGE #2.2 "}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT cast(to_timestamp(date_trunc('second','2025-12-15T08:27:34Z'::timestamp)) as date); "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"890e50a8-a0da-4318-af34-1ed4c4f19c40","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["2025-12-15"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"CAST(to_timestamp(date_trunc(second, 2025-12-15T08:27:34Z)) AS DATE)","type":"\"date\"","metadata":"{\"__autoGeneratedAlias\":\"true\"}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CAST(to_timestamp(date_trunc(second, 2025-12-15T08:27:34Z)) AS DATE)</th></tr></thead><tbody><tr><td>2025-12-15</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"challenge","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":882737275823811,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":882737275823809}},"nbformat":4,"nbformat_minor":0}
