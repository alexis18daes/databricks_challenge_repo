{"cells":[{"cell_type":"code","source":["%sql\nCREATE DATABASE IF NOT EXISTS db_bronze;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"implicitDf":true},"nuid":"8a0d9f4c-d4e1-451b-ba6b-ac0a7899d881","inputWidgets":{},"title":"CREAR BASE DE DATOS"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nDROP TABLE IF EXISTS db_bronze.hired_employees;\nCREATE TABLE db_bronze.hired_employees(\n  id bigint,\n  name string,\n  datetime string,\n  department_id bigint,\n  job_id bigint\n);\n\nDROP TABLE IF EXISTS db_bronze.departments;\nCREATE TABLE db_bronze.departments(\n  id bigint,\n  department string\n);\n\nDROP TABLE IF EXISTS db_bronze.jobs;\nCREATE TABLE db_bronze.jobs(\n  id bigint,\n  job string\n)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"implicitDf":true},"nuid":"9eddcb7e-ab05-46bc-8a58-2000c14738a1","inputWidgets":{},"title":"CREACION DE TABLAS"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%python\n# coding: utf-8\n\n# ||********************************************************************************************************\n# || PROYECTO   \t\t: POC -CHALLENGE GLOBLANT \n# || NOMBRE     \t\t: challenge.py\n# || TABLA DESTINO\t: db_bronze.hired_employees\n# ||                  db_bronze.departments \n# ||                  db_bronze.jobs\n# || TABLAS FUENTES\t: departments.csv\n# ||                  hired_employees.csv\n# ||                  jobs.csv\n# || OBJETIVO   \t\t: ETL - big data migrati\n# || TIPO       \t\t: pyspark\n# || REPROCESABLE\t  : NA\n# || SCHEDULER\t\t  : NA\n# || JOB  \t\t      : NA\n# || VERSION   DESARROLLADOR           FECHA        DESCRIPCION\n# || 1.1       ALEXIS DAVILA        21/03/23     Creacion del proceso\n# *************************************************************************************************************\n\n###\n # @section Import\n ##\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType,DoubleType,FloatType,LongType\n\n###\n # @section configuracion de recursos\n ##\nspark = SparkSession.builder.appName(\"CSV to Database\").getOrCreate()\n\n###\n # @section funciones\n ##\n\n\ndef insert_csv_to_db(csv_file,table_name,nombre_columnas):\n    \"\"\"\"\n    Esta función inserta data de archivos csv a una base de datos especificando las columnas.\n    :param csv_file: archivos csv\n    :param table_name: nombre y esquema de la tabla en donde se inserta la data\n    :param nombre_columnas: la lista de columnas de la tabla\n    \"\"\"\n    # Lee la tabla y guarda su contenido en un DataFrame\n    data_test = pd.read_csv(csv_file,sep=',', header='infer')\n    df=spark.createDataFrame(data_test)\n    nombres_columnas_final = nombre_columnas\n    #Asigna las columnas en el dataframe\n    df = df.toDF(*nombres_columnas_final)\n    department_id='department_id'\n    if department_id in nombres_columnas_final:\n        df = df.withColumn(\"department_id\",col(\"department_id\").cast(LongType()))\n        df = df.withColumn(\"job_id\",col(\"job_id\").cast(LongType()))\n    else:\n        print('Argumento \"valida id\" no válido')\n    # Guarda el DataFrame en formato delta en la la tabla especificada\n    df.show()\n    df.write.format(\"delta\").mode(\"append\").option(\"batchsize\", \"1000\").option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n\ndef backup_table(table_name, backup_path, backup_format=\"avro\"):\n    \"\"\"\n    Esta función crea una copia de seguridad de una tabla en formato AVRO y la guarda en el sistema de archivos.\n    :param table_name: nombre de la tabla a respaldar\n    :param backup_path: ruta donde se guardará el archivo de respaldo\n    :param backup_format: formato en que se guardará el archivo de respaldo. Por defecto es \"avro\".\n    \"\"\"\n    # Lee la tabla y guarda su contenido en un DataFrame\n    df = spark.table(table_name)\n    # Guarda el DataFrame en formato AVRO en la ruta especificada\n    df.write.format(backup_format).save(backup_path)\n    \ndef restore_table(table_name, backup_path, backup_format=\"avro\"):\n    \"\"\"\n    Esta función restaura una tabla a partir de su copia de seguridad en formato AVRO.\n    :param table_name: nombre de la tabla a restaurar\n    :param backup_path: ruta donde se encuentra el archivo de copia de seguridad\n    :param backup_format: formato del archivo de copia de seguridad. Por defecto es \"avro\".\n    \"\"\"\n    # Lee el archivo de copia de seguridad en formato AVRO y carga su contenido en un DataFrame\n    df = spark.read.format(backup_format).load(backup_path)\n    # Escribe el contenido del DataFrame en la tabla especificada\n    df.write.mode(\"overwrite\").saveAsTable(table_name)\n\ndef main():\n    \n    csv_file_1 = \"https://raw.githubusercontent.com/alexis18daes/databricks_challenge_repo/dev/hired_employees.csv\" \n    csv_file_2 = \"https://raw.githubusercontent.com/alexis18daes/databricks_challenge_repo/dev/departments.csv\"\n    csv_file_3 = \"https://raw.githubusercontent.com/alexis18daes/databricks_challenge_repo/dev/jobs.csv\"\n    \n    table_name_1 = \"db_bronze.hired_employees\"\n    table_name_2 = \"db_bronze.departments\"\n    table_name_3 = \"db_bronze.jobs\"\n    \n    nombre_columnas_1 = [\"id\",\"name\",\"datetime\",\"department_id\",\"job_id\"]\n    nombre_columnas_2 = [\"id\",\"name\",\"department\"]\n    nombre_columnas_3 = [\"id\",\"job\"]\n    \n    insert_csv_to_db(csv_file_1,table_name_1,nombre_columnas_1)\n    insert_csv_to_db(csv_file_2,table_name_2,nombre_columnas_2)\n    insert_csv_to_db(csv_file_3,table_name_3,nombre_columnas_3)\n    \n#Ejecucion\nmain()\n\nspark.stop()\n\n#Salida\nexit()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"c1b54a92-42e6-4612-84d2-982adf2f02eb","inputWidgets":{},"title":"PROCESO PRINCIPAL"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------------+--------------------+-------------+------+\n| id|              name|            datetime|department_id|job_id|\n+---+------------------+--------------------+-------------+------+\n|  2|          Ty Hofer|2021-05-30T05:43:46Z|            8|  null|\n|  3|       Lyman Hadye|2021-09-01T23:27:38Z|            5|    52|\n|  4|     Lotti Crowthe|2021-10-01T13:04:21Z|           12|    71|\n|  5|    Gretna Lording|2021-10-10T22:22:17Z|            6|    80|\n|  6|    Marlow Antecki|2021-04-23T23:45:42Z|            6|    95|\n|  7|      Joan Rillett|2021-10-10T01:33:31Z|            9|    78|\n|  8|  Ulrick Nucciotti|2021-07-24T01:28:40Z|            8|   169|\n|  9|Lucretia Northcote|2021-04-01T21:22:47Z|            9|     8|\n| 10|      Arty Giacobo|2022-02-08T12:27:07Z|            6|    62|\n| 11|      Libbi Dowtry|2021-07-05T04:55:10Z|            6|    41|\n| 12|      Jacky Oldred|2021-06-20T11:59:41Z|            7|    29|\n| 13|      Raine Mowett|2021-12-23T08:23:34Z|            8|    83|\n| 14|   Melonie Slocomb|2021-12-30T23:08:59Z|           12|   121|\n| 15|    Robers Swinden|2021-02-16T20:00:38Z|            5|    47|\n| 16|     Bone Serridge|2021-06-21T09:11:06Z|            8|   121|\n| 17|      Andee Tillot|2021-04-01T10:28:22Z|            8|    65|\n| 18|       Gay Philbin|2021-12-24T13:40:11Z|            6|   102|\n| 19|    Loralie Dundin|2021-10-19T20:10:46Z|            8|    68|\n| 20|       Tobi Lawton|2022-02-20T07:46:22Z|            6|    85|\n| 21|      Mandel Nayer|2021-12-18T22:55:26Z|            2|   121|\n+---+------------------+--------------------+-------------+------+\nonly showing top 20 rows\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-882737275823810>\u001B[0m in \u001B[0;36m<cell line: 105>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[0;31m#Ejecucion\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 105\u001B[0;31m \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    106\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m<command-882737275823810>\u001B[0m in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[0minsert_csv_to_db\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_file_1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtable_name_1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnombre_columnas_1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 101\u001B[0;31m     \u001B[0minsert_csv_to_db\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_file_2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtable_name_2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnombre_columnas_2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    102\u001B[0m     \u001B[0minsert_csv_to_db\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_file_3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtable_name_3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnombre_columnas_3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m<command-882737275823810>\u001B[0m in \u001B[0;36minsert_csv_to_db\u001B[0;34m(csv_file, table_name, nombre_columnas)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[0mnombres_columnas_final\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnombre_columnas\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[0;31m#Asigna las columnas en el dataframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoDF\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mnombres_columnas_final\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m     \u001B[0mdepartment_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'department_id'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mdepartment_id\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mnombres_columnas_final\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mtoDF\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   3437\u001B[0m         \u001B[0;34m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Alice'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Bob'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3438\u001B[0m         \"\"\"\n\u001B[0;32m-> 3439\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoDF\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jseq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3440\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3441\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: The number of columns doesn't match.\nOld column names (2): 1, Product Management\nNew column names (3): id, name, department","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: The number of columns doesn't match.\nOld column names (2): 1, Product Management\nNew column names (3): id, name, department","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-882737275823810>\u001B[0m in \u001B[0;36m<cell line: 105>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[0;31m#Ejecucion\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 105\u001B[0;31m \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    106\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m<command-882737275823810>\u001B[0m in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[0minsert_csv_to_db\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_file_1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtable_name_1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnombre_columnas_1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 101\u001B[0;31m     \u001B[0minsert_csv_to_db\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_file_2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtable_name_2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnombre_columnas_2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    102\u001B[0m     \u001B[0minsert_csv_to_db\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_file_3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtable_name_3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnombre_columnas_3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m<command-882737275823810>\u001B[0m in \u001B[0;36minsert_csv_to_db\u001B[0;34m(csv_file, table_name, nombre_columnas)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[0mnombres_columnas_final\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnombre_columnas\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[0;31m#Asigna las columnas en el dataframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoDF\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mnombres_columnas_final\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m     \u001B[0mdepartment_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'department_id'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mdepartment_id\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mnombres_columnas_final\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mtoDF\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   3437\u001B[0m         \u001B[0;34m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Alice'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Bob'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3438\u001B[0m         \"\"\"\n\u001B[0;32m-> 3439\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoDF\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jseq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3440\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3441\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: The number of columns doesn't match.\nOld column names (2): 1, Product Management\nNew column names (3): id, name, department"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"challenge","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":882737275823816,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":882737275823809}},"nbformat":4,"nbformat_minor":0}
